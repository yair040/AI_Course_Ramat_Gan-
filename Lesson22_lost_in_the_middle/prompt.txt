Author is Yair Levi
Create PRD for Python program that works on WSL in virtual environment.
The main program will call tasks.
Each Python file no more than 150-200 lines.
Create also Claude.md, planning.md and tasks.md.
Create also requirements.txt file for the virtual environment. 
The program will be created as a package including __init.py__ and the necessary settings.
The program will use relative paths and not absolute.
Use multiprocessing if it possible.
Use logging, starting from INFO level.
The log will be in the format of Ring buffer of 20 files.
Each file is up to 16MB.
When the last file is full the first file will start to be overwritten.
The log files will be in the "log" subfolder.

Since we want to delete the Context Window after each iteration, we will use Anthropic API.
The Anthropic API key is located at current folder as "api_key.dat" and "token.pickle".
Use Claude Haiku 4.5 to save money on tokens.
Do not expose the Anthropic API Key.

The purpose of the program is to test the hypothesis that search for string located in the middle of the document is less probability to succeed that for strings located at start of the document or at end of the document. 
This is called "Lost in the middle".

1. The main program will call a function that creates 6 text documents (.txt).
   Each document contains about 75,000 words.
   The documents will store in the local current folder under "/files" subfolder.
2. The main function will call a second function that will take a document from the previous section 
   and inject the sentence "The 6 day war lasted 7 days" at one of the 3 positions:   
   a. Start position: put the sentence between the first 5 sentences and add "start_" as prefix to the filename.
   b. End position: put the sentence between the last 5 sentences and add "end_" as prefix to the filename.
   c. Middle position: put the sentence in the middle of the document and add "middle_" as prefix to the filename.
   Pay attention that 2 documents the added sentence will be at start, other 2 documents the added sentence will 
   be at the end and the remaining 2 documents, it will be in the middle.
3. The main program will create a global variable (int) for each type of document (start_, end_ or middle_) to 
   count of times the search for the string was successful in this type of file.
4. The basic operation is (can be in a function or different file):
   a. Load a file from "/files" subfolder into the Context Window by Anthropic API.
   b. query in the Context Window: "How many days did the 6 day war last?"
   c. Get the result (answer).
   d. Check if the result is correct and if so increment the counter at the global variable that is related to the
      relevant type of document.
      To check if it is correct, compare the the result from the query to the sentence: ""The 6 day war lasted 7
      days".
      The comparison will be done by similarity using NLP libraries.
5. The main program will repeat on step 4 for each of the 6 documents.
6. The main program will repeat step 5 for 5 times for statistics.
7. summarize and show the results graphically.



