Author is Yair Levi
Create PRD for Python program that works on WSL in virtual environment.
The main program will call tasks.
Each Python file no more than 150-200 lines.
Create also Claude.md, planning.md and tasks.md.
Create also requirements.txt file for the virtual environment. 
The program will be created as a package including __init.py__ and the necessary settings.
The program will use relative paths and not absolute.
Use multiprocessing if it possible.
Use logging, starting from INFO level.
The log will be in the format of Ring buffer of 20 files.
Each file is up to 16MB.
When the last file is full the first file will start to be overwritten.
The log files will be in the "log" subfolder.

Since we want to delete the Context Window after each iteration, we will use Anthropic API.
The Anthropic API key is located at current folder as "api_key.dat".
Use Claude Haiku 4.5 to save money on tokens.
Do not expose the Anthropic API Key.

The purpose of the program is to test the hypothesis that search for string located in the the 
document is less probability to succeed as the document size in the context window is bigger than a specific size . 
The hipothesis is that from some document size the answer will not we correct.

1. The main program will call a function that creates 7 English text documents (.txt).
   In each document insert in its middle the sentence "The first prime minister of Israel was Ben Gurion".
   The first document contains 2000 words, the second 5000 words, the third 10000 words, the forth 20000
   words, the fifth 30000 words, the sixth 40000 words and the seventh (last) 50000 words.
   The documents will store in the local current folder under "/files" subfolder.
2. The main program will create a global variable (list) that will contains dictionary for each document 
   (The list  will contain 7 dictionaries).
   Each dictionary contains:
   a. Time took to process the query on the document.
   b. Accuracy of the response to the query.
   c. Size of used Context Window.
4. The basic operation is (can be in a function or different file):
   a. Load a document from "/files" subfolder into the Context Window by Anthropic API.
   b. query in the Context Window: "Who was the first Prime Minister of Israel?"
   c. Get the result (answer).
   d. Measure the time from start of query till getting the result and save it as a value in the Time key 
      in the dictionary that is related to this document at the global list variable.
   e. Check if the result is correct and if so put 1 as a value in the Accuracy key in the dictionary 
      that is related to this document at the global list variable.
      If not correct, put there 0.       
      To check if it is correct, compare the result from the query to the sentence: "Ben Gurion was the first
      Prime Minister of Israel".
      The comparison will be done by similarity using NLP libraries.
   f. Check the size of the document in tokens and save it as a value in the Size key in the dictionary 
      that is related to this document at the global list variable.
5. The main program will repeat on step 4 for each of the 7 documents and fill the list of dictionaries
   accordingly.
7. summarize and show the results graphically.
   A graph that will show: 
   X axis: number of tokens in document.
   Y axis: Execute query time and response accuracy. 

